{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "pUSNRoe1zJ5r",
        "outputId": "3c249fe8-5ec9-4625-f4b5-b41a1d6db9bd"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 127] The specified procedure could not be found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Vectors, Vocab\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSastrawi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStemmer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStemmerFactory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StemmerFactory\n",
            "File \u001b[1;32mc:\\Users\\kusmo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\kusmo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\kusmo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
            "File \u001b[1;32mc:\\Users\\kusmo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\kusmo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_ops.py:1357\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1352\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1357\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
            "File \u001b[1;32mc:\\Users\\kusmo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from xgboost import XGBRFClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import Vectors, Vocab\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypGvx2VSl0sp"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV4uUYkAlyZV"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuKsyj5CzJ5v"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"indonesian\"))\n",
        "def bersihkan_teks(teks):\n",
        "    teks = teks.lower()\n",
        "    teks = re.sub(r'\\d+', '', teks)\n",
        "    teks = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", teks, flags=re.MULTILINE)\n",
        "    teks = re.sub(r\"@\\w+|#\", \"\", teks)\n",
        "    teks = re.sub(r\"[^\\w\\s]\", \"\", teks)\n",
        "    teks = re.sub(r'<.*?>', '', teks)\n",
        "    teks = re.sub(r'\\s+', ' ', teks).strip()\n",
        "    tokens = \" \".join([stemmer.stem(word) for word in teks.split() if word not in stop_words])\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNFjZoVPa27s"
      },
      "outputs": [],
      "source": [
        "csv_file = \"dataset_pengaduan.csv\"\n",
        "folder_data = \"dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lWdYbDczJ5y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1juhqA7zJ50"
      },
      "outputs": [],
      "source": [
        "df['konten'] = df['pengaduan'].apply(bersihkan_teks)\n",
        "df['pengaduan'] = df['konten']\n",
        "df.drop(columns=['pengaduan'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0b-6fnRzJ51"
      },
      "outputs": [],
      "source": [
        "df_classification = df[['date','konten', 'kategori', 'sentimen']]\n",
        "df_classification.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kApRfJNZauSq"
      },
      "outputs": [],
      "source": [
        "TEKS = Field(sequential=True, tokenize=lambda x: x.split(), lower=True, include_lengths=False, batch_first=True)\n",
        "KATEGORI = Field(sequential=False, use_vocab=False)\n",
        "LABEL = Field(sequential=False, use_vocab=False)\n",
        "TANGGAL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "fields = [\n",
        "    ('date', TANGGAL),\n",
        "    ('konten', TEKS),\n",
        "    ('kategori', KATEGORI),\n",
        "    ('sentimen', LABEL)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P31UxcBY7RX2"
      },
      "source": [
        "# Pre-proccessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mWj62EH7e8P"
      },
      "source": [
        "## pre-sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2wmGZPzJ53"
      },
      "source": [
        "Ekstrasi fitur menggunakan TF-IDF untuk menghitung nilai dari kata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_9U_TpkzJ54"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "konten_tfidf = tfidf.fit_transform(df_classification['konten']).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPvIowpzJ55"
      },
      "source": [
        "Mengubah kategori menjadi label agar bisa digabung untuk meningkatkan kualitas training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Lmqd1IKVpM"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder()\n",
        "kategori_encoded = ohe.fit_transform(df_classification[['kategori']]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI3KVoXUzJ56"
      },
      "source": [
        "Mengubah sentimen menjadi label, agar bisa digunakan untuk prediksi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw5ceaiSzJ56"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g7qsgY_zJ57"
      },
      "source": [
        "# Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qjt4bB8jds"
      },
      "source": [
        "## Split for sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaKWtM0JKVpR"
      },
      "outputs": [],
      "source": [
        "feature = np.hstack((konten_tfidf, kategori_encoded, date_features_scaled))\n",
        "target = le.fit_transform(df_classification['sentimen'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqHt7tlqzJ57"
      },
      "outputs": [],
      "source": [
        "X = feature\n",
        "y = target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOYFdY8OzJ58"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mLnIr1h8ohy"
      },
      "source": [
        "## Split for Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzPp19QbKVpS"
      },
      "source": [
        "### Ekstraksi Fitur khusus deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSA9ZRj_KVpS"
      },
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_uvujqVKVpT"
      },
      "outputs": [],
      "source": [
        "df_train, df_val = train_test_split(df_classification, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5gKOxrrKVpT"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv(f\"{folder_data}train_dataset.csv\", index=False)\n",
        "df_val.to_csv(f\"{folder_data}val_dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgTnK2XyKVpT"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = TabularDataset.splits(\n",
        "    path=folder_data,\n",
        "    train='train_dataset.csv',\n",
        "    validation='val_dataset.csv',\n",
        "    format='csv',\n",
        "    fields=fields,\n",
        "    skip_header=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLLuk_dgKVpT"
      },
      "outputs": [],
      "source": [
        "print(f\"Jumlah data train: {len(train_data)}\")\n",
        "print(f\"Jumlah data val: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz76HnXZKVpU"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz\n",
        "!gunzip cc.id.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWBpVfsPKVpU"
      },
      "outputs": [],
      "source": [
        "vectors = Vectors(name='cc.id.300.bin', cache='./.vector_cache')\n",
        "max_words = 10000\n",
        "TEKS.build_vocab(train_data, max_size=max_words, vectors=vectors, unk_init=torch.Tensor.normal_)\n",
        "\n",
        "print(f\"Ukuran vocab: {len(TEKS.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dogiB59ldaBx"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder()\n",
        "kategori_train = ohe.fit_transform(df_train[['kategori']])\n",
        "kategori_val = ohe.transform(df_val[['kategori']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_PmFLSSKVpV"
      },
      "outputs": [],
      "source": [
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "df_val['date'] = pd.to_datetime(df_val['date'])\n",
        "\n",
        "date_features_train = np.array([\n",
        "    df_train['date'].dt.year,\n",
        "    df_train['date'].dt.month,\n",
        "    df_train['date'].dt.day,\n",
        "    df_train['date'].dt.weekday\n",
        "]).T\n",
        "\n",
        "date_features_val = np.array([\n",
        "    df_val['date'].dt.year,\n",
        "    df_val['date'].dt.month,\n",
        "    df_val['date'].dt.day,\n",
        "    df_val['date'].dt.weekday\n",
        "]).T\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "date_features_train_scaled = scaler.fit_transform(date_features_train)\n",
        "date_features_val_scaled = scaler.transform(date_features_val)\n",
        "\n",
        "print(\"date_features_train_scaled shape:\", date_features_train_scaled.shape)\n",
        "print(\"date_features_val_scaled shape:\", date_features_val_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-IkjQyxKVpV"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_nn = label_encoder.fit_transform(df_train['sentimen'])\n",
        "y_val = label_encoder.transform(df_val['sentimen'])\n",
        "\n",
        "print(\"y_train shape:\", y_train_nn.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hjf8qacKVpV"
      },
      "source": [
        "### Pengacakan data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPNglPWlfFeN"
      },
      "outputs": [],
      "source": [
        "accelerator = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV39DTnKzJ58"
      },
      "outputs": [],
      "source": [
        "train_iter, val_iter = BucketIterator.splits(\n",
        "    (train_data, val_data),\n",
        "    batch_size=64,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.konten),\n",
        "    device=accelerator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfs5wufdKVpW"
      },
      "outputs": [],
      "source": [
        "def batch(batch, kategori_data, date_data, labels):\n",
        "    sequences = batch.konten\n",
        "    kategori = torch.FloatTensor(kategori_data[batch.batch_idx]).to(accelerator)\n",
        "    date_features = torch.FloatTensor(date_data[batch.batch_idx]).to(accelerator)\n",
        "    labels = torch.LongTensor(labels[batch.batch_idx]).to(accelerator)\n",
        "    return (sequences, kategori, date_features), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ax1HJ1jKVpf"
      },
      "outputs": [],
      "source": [
        "train_iter.batch_idx = np.arange(len(train_iter.dataset))\n",
        "val_iter.batch_idx = np.arange(len(val_iter.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrxe6XAbf3z9"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = TEKS.vocab.vectors\n",
        "embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "print(\"Shape:\", embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwrnVSwU-lcO"
      },
      "source": [
        "# Pembuatan Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOK5IOfq-ouz"
      },
      "source": [
        "## Model sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrVxufkYzJ59"
      },
      "outputs": [],
      "source": [
        "model_xgb = XGBRFClassifier()\n",
        "model_rf = RandomForestClassifier(class_weight='balanced', random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj9jSFf5-uU_"
      },
      "source": [
        "## Model Deep Learning menggunakan Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZBScPc6-tXS"
      },
      "outputs": [],
      "source": [
        "class lstm_model(nn.Module):\n",
        "    def __init__(self, embedding_matrix, max_len, num_kategori, num_date, hidden_dim=256, num_classes=3, dropout=0.3):\n",
        "        super(lstm_model, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix), freeze=False\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_matrix.shape[1],\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=3,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc_lstm = nn.Linear(hidden_dim * 2, 128)\n",
        "        self.fc_additional = nn.Linear(num_kategori + num_date, 364)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 + 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        sequences, kategori, date_features = inputs\n",
        "\n",
        "        embedded = self.embedding(sequences)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "\n",
        "        lstm_out = self.fc_lstm(context)\n",
        "\n",
        "        additional = torch.cat((kategori, date_features), dim=1)\n",
        "        additional = self.fc_additional(additional)\n",
        "\n",
        "        combined = torch.cat((lstm_out, additional), dim=1)\n",
        "        out = self.fc(combined)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Iyl9IZNOyDW"
      },
      "source": [
        "Balancing sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8haNIZvNn4P"
      },
      "outputs": [],
      "source": [
        "print(df_classification['sentimen'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GES20Qn6O2S0"
      },
      "outputs": [],
      "source": [
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ltx5Wf4bON8"
      },
      "outputs": [],
      "source": [
        "# for key in class_weight_dict:\n",
        "#     class_weight_dict[key] = min(class_weight_dict[key], 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tLBWKMabPL4"
      },
      "outputs": [],
      "source": [
        "print(class_weight_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9maYoce_FaN"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyl77DSQ_K6J"
      },
      "source": [
        "## Model sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3JOs2knzJ5-"
      },
      "outputs": [],
      "source": [
        "#Training Model XGBoost\n",
        "model_xgb.fit(X_train, y_train)\n",
        "y_train_predic_xgb = model_xgb.predict(X_train)\n",
        "y_test_predic_xgb = model_xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyj1kpI2zJ5-"
      },
      "outputs": [],
      "source": [
        "train_accuracy_xgb = accuracy_score(y_train, y_train_predic_xgb)\n",
        "test_accuracy_xgb = accuracy_score(y_test, y_test_predic_xgb)\n",
        "\n",
        "print(\"XGBoost:\")\n",
        "print(f\"Akurasi Training: {train_accuracy_xgb:.4f}\")\n",
        "print(f\"Akurasi Testing: {test_accuracy_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0xopxByzJ5_"
      },
      "outputs": [],
      "source": [
        "#Training Model Random Forest\n",
        "model_rf.fit(X_train, y_train)\n",
        "y_train_predic_rf = model_rf.predict(X_train)\n",
        "y_test_predic_rf = model_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY4Ylp9BzJ5_"
      },
      "outputs": [],
      "source": [
        "train_accuracy_rf = accuracy_score(y_train, y_train_predic_rf)\n",
        "test_accuracy_rf = accuracy_score(y_test, y_test_predic_rf)\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(f\"Akurasi Training: {train_accuracy_rf:.4f}\")\n",
        "print(f\"Akurasi Testing: {test_accuracy_rf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BOOyZRi_QqC"
      },
      "source": [
        "## Model Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fnCGTX5zJ5_"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "num_kategori = kategori_train.shape[1]\n",
        "num_tanggal = date_features_train_scaled.shape[1]\n",
        "\n",
        "deep_model = lstm_model(\n",
        "    embedding_matrix=embedding_matrix,\n",
        "    max_len=max_len,\n",
        "    num_kategori=num_kategori,\n",
        "    num_date=num_tanggal,\n",
        "    hidden_dim=256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3fp9WkvKVpr"
      },
      "outputs": [],
      "source": [
        "deep_model = deep_model.to(accelerator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHz0dgTBzJ6A"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.AdamW(deep_model.parameters(), lr=0.001, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IJ1XvetzJ6A"
      },
      "outputs": [],
      "source": [
        "class_weights= torch.FloatTensor(class_weights).to(accelerator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y47Q6MfrzJ6B"
      },
      "outputs": [],
      "source": [
        "def training(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in train_iter:\n",
        "            inputs, labels = prepare_batch(batch, kategori_train, date_features_train_scaled, y_train)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_iter)\n",
        "        epoch_acc = 100 * correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_iter:\n",
        "                inputs, labels = prepare_batch(batch, kategori_val, date_features_val_scaled, y_val)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_iter)\n",
        "        epoch_val_acc = 100 * val_correct / val_total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, '\n",
        "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGefF2WYKVps"
      },
      "outputs": [],
      "source": [
        "train_losses, train_accs, val_losses, val_accs = training(\n",
        "    deep_model,\n",
        "    train_iter,\n",
        "    val_iter,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kvv_VblD27Qs"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, sequences, kategori, date_features, labels, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sequences = torch.LongTensor(sequences).to(device)\n",
        "        kategori = torch.FloatTensor(kategori).to(device)\n",
        "        date_features = torch.FloatTensor(date_features).to(device)\n",
        "        labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "        inputs = (sequences, kategori, date_features)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        accuracy = (predicted == labels).sum().item() / len(labels)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "train_sequences = []\n",
        "val_sequences = []\n",
        "\n",
        "for example in train_data:\n",
        "    train_sequences.append([TEXT.vocab.stoi[word] for word in example.konten])\n",
        "for example in val_data:\n",
        "    val_sequences.append([TEXT.vocab.stoi[word] for word in example.konten])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxnxaR8VKVpt"
      },
      "outputs": [],
      "source": [
        "train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "test_accuracy = evaluate_model(\n",
        "    deep_model,\n",
        "    val_sequences,\n",
        "    kategori_val,\n",
        "    date_features_val_scaled,\n",
        "    y_val,\n",
        "    accelerator\n",
        ")\n",
        "\n",
        "print(f'Validation Accuracy: {test_accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ICaK1L6KVpu"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(train_losses, train_accs, val_loss, val_acc):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train', color='black')\n",
        "    plt.plot(val_loss, label='Validation', color='purple')\n",
        "    plt.title('Loss per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train', color='black')\n",
        "    plt.plot(val_acc, label='Validation', color='purple')\n",
        "    plt.title('Accuracy per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(train_losses, train_accur, val_loss, val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
